GPGPU-Sim version 4.2.0 (build gpgpu-sim_git-commit-13c67115070dc2f0876254a790d0238073ca364a-modified_590.0) configured with AccelWattch.

----------------------------------------------------------------------------
INFO - If you only care about PTX execution, ignore this message. GPGPU-Sim supports PTX execution in modern CUDA.
If you want to run PTXPLUS (sm_1x SASS) with a modern card configuration - set the envronment variable
$PTXAS_CUDA_INSTALL_PATH to point a CUDA version compabible with your card configurations (i.e. 8+ for PASCAL, 9+ for VOLTA etc..)
For example: "export $PTXAS_CUDA_INSTALL_PATH=/usr/local/cuda-9.1"

The following text describes why:
If you are using PTXPLUS, only sm_1x is supported and it requires that the app and simulator binaries are compiled in CUDA 4.2 or less.
The simulator requires it since CUDA headers desribe struct sizes in the exec which change from gen to gen.
The apps require 4.2 because new versions of CUDA tools have dropped parsing support for generating sm_1x
When running using modern config (i.e. volta) and PTXPLUS with CUDA 4.2, the $PTXAS_CUDA_INSTALL_PATH env variable is required to get proper register usage
(and hence occupancy) using a version of CUDA that knows the register usage on the real card.

----------------------------------------------------------------------------
setup_environment succeeded
Accel-Sim [build accelsim-commit-e02c99dbadefc0b9dc95317100be2a446eec142c_modified_0.0]

        *** GPGPU-Sim Simulator Version 4.2.0  [build gpgpu-sim_git-commit-13c67115070dc2f0876254a790d0238073ca364a_modified_0.0] ***


GPGPU-Sim: Configuration options:

-save_embedded_ptx                      0 # saves ptx files embedded in binary as <n>.ptx
-keep                                   0 # keep intermediate files created by GPGPU-Sim when interfacing with external programs
-gpgpu_ptx_save_converted_ptxplus                    0 # Saved converted ptxplus to a file
-gpgpu_occupancy_sm_number                   60 # The SM number to pass to ptxas when getting register usage for computing GPU occupancy. This parameter is required in the config.
-ptx_opcode_latency_int         4,13,4,5,145 # Opcode latencies for integers <ADD,MAX,MUL,MAD,DIV,SHFL>Default 1,1,19,25,145,32
-ptx_opcode_latency_fp          4,13,4,5,39 # Opcode latencies for single precision floating points <ADD,MAX,MUL,MAD,DIV>Default 1,1,1,1,30
-ptx_opcode_latency_dp         8,19,8,8,330 # Opcode latencies for double precision floating points <ADD,MAX,MUL,MAD,DIV>Default 8,8,8,8,335
-ptx_opcode_latency_sfu                    8 # Opcode latencies for SFU instructionsDefault 8
-ptx_opcode_latency_tesnor                   64 # Opcode latencies for Tensor instructionsDefault 64
-ptx_opcode_initiation_int            1,2,2,2,8 # Opcode initiation intervals for integers <ADD,MAX,MUL,MAD,DIV,SHFL>Default 1,1,4,4,32,4
-ptx_opcode_initiation_fp            1,2,1,1,4 # Opcode initiation intervals for single precision floating points <ADD,MAX,MUL,MAD,DIV>Default 1,1,1,1,5
-ptx_opcode_initiation_dp          1,2,1,1,130 # Opcode initiation intervals for double precision floating points <ADD,MAX,MUL,MAD,DIV>Default 8,8,8,8,130
-ptx_opcode_initiation_sfu                    8 # Opcode initiation intervals for sfu instructionsDefault 8
-ptx_opcode_initiation_tensor                   64 # Opcode initiation intervals for tensor instructionsDefault 64
-cdp_latency         7200,8000,100,12000,1600 # CDP API latency <cudaStreamCreateWithFlags, cudaGetParameterBufferV2_init_perWarp, cudaGetParameterBufferV2_perKernel, cudaLaunchDeviceV2_init_perWarp, cudaLaunchDevicV2_perKernel>Default 7200,8000,100,12000,1600
-network_mode                           1 # Interconnection network mode
-inter_config_file   config_fermi_islip.icnt # Interconnection network config file
-icnt_in_buffer_limit                   64 # in_buffer_limit
-icnt_out_buffer_limit                   64 # out_buffer_limit
-icnt_subnets                           2 # subnets
-icnt_arbiter_algo                      1 # arbiter_algo
-icnt_verbose                           0 # inct_verbose
-icnt_grant_cycles                      1 # grant_cycles
-gpgpu_ptx_use_cuobjdump                    1 # Use cuobjdump to extract ptx and sass from binaries
-gpgpu_experimental_lib_support                    0 # Try to extract code from cuda libraries [Broken because of unknown cudaGetExportTable]
-checkpoint_option                      0 #  checkpointing flag (0 = no checkpoint)
-checkpoint_kernel                      1 #  checkpointing during execution of which kernel (1- 1st kernel)
-checkpoint_CTA                         0 #  checkpointing after # of CTA (< less than total CTA)
-resume_option                          0 #  resume flag (0 = no resume)
-resume_kernel                          0 #  Resume from which kernel (1= 1st kernel)
-resume_CTA                             0 #  resume from which CTA 
-checkpoint_CTA_t                       0 #  resume from which CTA 
-checkpoint_insn_Y                      0 #  resume from which CTA 
-gpgpu_ptx_convert_to_ptxplus                    0 # Convert SASS (native ISA) to ptxplus and run ptxplus
-gpgpu_ptx_force_max_capability                   60 # Force maximum compute capability
-gpgpu_ptx_inst_debug_to_file                    0 # Dump executed instructions' debug information to file
-gpgpu_ptx_inst_debug_file       inst_debug.txt # Executed instructions' debug output file
-gpgpu_ptx_inst_debug_thread_uid                    1 # Thread UID for executed instructions' debug output
-gpgpu_simd_model                       1 # 1 = post-dominator
-gpgpu_shader_core_pipeline              2048:32 # shader core pipeline config, i.e., {<nthread>:<warpsize>}
-gpgpu_tex_cache:l1  N:16:128:24,L:R:m:N:L,F:128:4,128:2 # per-shader L1 texture cache  (READ-ONLY) config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>:<rf>}
-gpgpu_const_cache:l1 N:128:64:2,L:R:f:N:L,A:2:64,4 # per-shader L1 constant memory cache  (READ-ONLY) config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} 
-gpgpu_cache:il1     N:8:128:4,L:R:f:N:L,A:2:48,4 # shader L1 instruction cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} 
-gpgpu_cache:dl1     N:64:128:6,L:L:m:N:H,A:128:8,8 # per-shader L1 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}
-gpgpu_l1_cache_write_ratio                    0 # L1D write ratio
-gpgpu_l1_banks                         1 # The number of L1 cache banks
-gpgpu_l1_banks_byte_interleaving                   32 # l1 banks byte interleaving granularity
-gpgpu_l1_banks_hashing_function                    0 # l1 banks hashing function
-gpgpu_l1_latency                       1 # L1 Hit Latency
-gpgpu_smem_latency                     3 # smem Latency
-gpgpu_cache:dl1PrefL1                 none # per-shader L1 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}
-gpgpu_cache:dl1PrefShared                 none # per-shader L1 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}
-gpgpu_gmem_skip_L1D                    1 # global memory access skip L1D cache (implements -Xptxas -dlcm=cg, default=no skip)
-gpgpu_perfect_mem                      0 # enable perfect memory mode (no cache miss)
-n_regfile_gating_group                    4 # group of lanes that should be read/written together)
-gpgpu_clock_gated_reg_file                    0 # enable clock gated reg file for power calculations
-gpgpu_clock_gated_lanes                    0 # enable clock gated lanes for power calculations
-gpgpu_shader_registers                65536 # Number of registers per shader core. Limits number of concurrent CTAs. (default 8192)
-gpgpu_registers_per_block                 8192 # Maximum number of registers per CTA. (default 8192)
-gpgpu_ignore_resources_limitation                    0 # gpgpu_ignore_resources_limitation (default 0)
-gpgpu_shader_cta                      32 # Maximum number of concurrent CTAs in shader (default 32)
-gpgpu_num_cta_barriers                   16 # Maximum number of named barriers per CTA (default 16)
-gpgpu_n_clusters                      20 # number of processing clusters
-gpgpu_n_cores_per_cluster                    1 # number of simd cores per cluster
-gpgpu_n_cluster_ejection_buffer_size                    8 # number of packets in ejection buffer
-gpgpu_n_ldst_response_buffer_size                    2 # number of response packets in ld/st unit ejection buffer
-gpgpu_shmem_per_block                49152 # Size of shared memory per thread block or CTA (default 48kB)
-gpgpu_shmem_size                   98304 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_option                     0 # Option list of shared memory sizes
-gpgpu_unified_l1d_size                    0 # Size of unified data cache(L1D + shared memory) in KB
-gpgpu_adaptive_cache_config                    0 # adaptive_cache_config
-gpgpu_shmem_sizeDefault                16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_size_PrefL1                16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_size_PrefShared                16384 # Size of shared memory per shader core (default 16kB)
-gpgpu_shmem_num_banks                   32 # Number of banks in the shared memory in each shader core (default 16)
-gpgpu_shmem_limited_broadcast                    0 # Limit shared memory to do one broadcast per cycle (default on)
-gpgpu_shmem_warp_parts                    1 # Number of portions a warp is divided into for shared memory bank conflict check 
-gpgpu_mem_unit_ports                    1 # The number of memory transactions allowed per core cycle
-gpgpu_shmem_warp_parts                    1 # Number of portions a warp is divided into for shared memory bank conflict check 
-gpgpu_warpdistro_shader                   -1 # Specify which shader core to collect the warp size distribution from
-gpgpu_warp_issue_shader                    0 # Specify which shader core to collect the warp issue distribution from
-gpgpu_local_mem_map                    1 # Mapping from local memory space address to simulated GPU physical address space (default = enabled)
-gpgpu_num_reg_banks                   32 # Number of register banks (default = 8)
-gpgpu_reg_bank_use_warp_id                    0 # Use warp ID in mapping registers to banks (default = off)
-gpgpu_sub_core_model                    0 # Sub Core Volta/Pascal model (default = off)
-gpgpu_enable_specialized_operand_collector                    1 # enable_specialized_operand_collector
-gpgpu_operand_collector_num_units_sp                   20 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_dp                    0 # number of collector units (default = 0)
-gpgpu_operand_collector_num_units_sfu                    4 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_int                    0 # number of collector units (default = 0)
-gpgpu_operand_collector_num_units_tensor_core                    4 # number of collector units (default = 4)
-gpgpu_operand_collector_num_units_mem                    8 # number of collector units (default = 2)
-gpgpu_operand_collector_num_units_gen                    0 # number of collector units (default = 0)
-gpgpu_operand_collector_num_in_ports_sp                    4 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_dp                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_in_ports_sfu                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_int                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_in_ports_tensor_core                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_mem                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_in_ports_gen                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_sp                    4 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_dp                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_sfu                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_int                    0 # number of collector unit in ports (default = 0)
-gpgpu_operand_collector_num_out_ports_tensor_core                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_mem                    1 # number of collector unit in ports (default = 1)
-gpgpu_operand_collector_num_out_ports_gen                    0 # number of collector unit in ports (default = 0)
-gpgpu_coalesce_arch                   13 # Coalescing arch (GT200 = 13, Fermi = 20)
-gpgpu_num_sched_per_core                    2 # Number of warp schedulers per core
-gpgpu_max_insn_issue_per_warp                    2 # Max number of instructions that can be issued per warp in one cycle by scheduler (either 1 or 2)
-gpgpu_dual_issue_diff_exec_units                    1 # should dual issue use two different execution unit resources (Default = 1)
-gpgpu_simt_core_sim_order                    1 # Select the simulation order of cores in a cluster (0=Fix, 1=Round-Robin)
-gpgpu_pipeline_widths 4,0,0,1,1,4,0,0,1,1,6 # Pipeline widths ID_OC_SP,ID_OC_DP,ID_OC_INT,ID_OC_SFU,ID_OC_MEM,OC_EX_SP,OC_EX_DP,OC_EX_INT,OC_EX_SFU,OC_EX_MEM,EX_WB,ID_OC_TENSOR_CORE,OC_EX_TENSOR_CORE
-gpgpu_tensor_core_avail                    0 # Tensor Core Available (default=0)
-gpgpu_num_sp_units                     4 # Number of SP units (default=1)
-gpgpu_num_dp_units                     0 # Number of DP units (default=0)
-gpgpu_num_int_units                    0 # Number of INT units (default=0)
-gpgpu_num_sfu_units                    1 # Number of SF units (default=1)
-gpgpu_num_tensor_core_units                    0 # Number of tensor_core units (default=1)
-gpgpu_num_mem_units                    1 # Number if ldst units (default=1) WARNING: not hooked up to anything
-gpgpu_scheduler                      gto # Scheduler configuration: < lrr | gto | two_level_active > If two_level_active:<num_active_warps>:<inner_prioritization>:<outer_prioritization>For complete list of prioritization values see shader.h enum scheduler_prioritization_typeDefault: gto
-gpgpu_concurrent_kernel_sm                    0 # Support concurrent kernels on a SM (default = disabled)
-gpgpu_perfect_inst_const_cache                    0 # perfect inst and const cache mode, so all inst and const hits in the cache(default = disabled)
-gpgpu_inst_fetch_throughput                    1 # the number of fetched intruction per warp each cycle
-gpgpu_reg_file_port_throughput                    1 # the number ports of the register file
-specialized_unit_1         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_2         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_3         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_4         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_5         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_6         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_7         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-specialized_unit_8         0,4,4,4,4,BRA # specialized unit config {<enabled>,<num_units>:<latency>:<initiation>,<ID_OC_SPEC>:<OC_EX_SPEC>,<NAME>}
-gpgpu_perf_sim_memcpy                    1 # Fill the L2 cache on memcpy
-gpgpu_simple_dram_model                    0 # simple_dram_model with fixed latency and BW
-gpgpu_dram_scheduler                    1 # 0 = fifo, 1 = FR-FCFS (defaul)
-gpgpu_dram_partition_queues              8:8:8:8 # i2$:$2d:d2$:$2i
-l2_ideal                               0 # Use a ideal L2 cache that always hit
-gpgpu_cache:dl2     N:64:128:16,L:B:m:W:L,A:1024:1024,4:0,32 # unified banked L2 data cache config  {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>}
-gpgpu_cache:dl2_texture_only                    0 # L2 cache used for texture only
-gpgpu_n_mem                            8 # number of memory modules (e.g. memory controllers) in gpu
-gpgpu_n_sub_partition_per_mchannel                    2 # number of memory subpartition in each memory module
-gpgpu_n_mem_per_ctrlr                    1 # number of memory chips per memory controller
-gpgpu_memlatency_stat                   14 # track and display latency statistics 0x2 enables MC, 0x4 enables queue logs
-gpgpu_frfcfs_dram_sched_queue_size                   64 # 0 = unlimited (default); # entries per chip
-gpgpu_dram_return_queue_size                  116 # 0 = unlimited (default); # entries per chip
-gpgpu_dram_buswidth                    4 # default = 4 bytes (8 bytes per cycle at DDR)
-gpgpu_dram_burst_length                    8 # Burst length of each DRAM request (default = 4 data bus cycle)
-dram_data_command_freq_ratio                    4 # Frequency ratio between DRAM data bus and command bus (default = 2 times, i.e. DDR)
-gpgpu_dram_timing_opt nbk=16:CCD=2:RRD=6:RCD=12:RAS=28:RP=12:RC=40: CL=12:WL=4:CDLR=5:WR=12:nbkgrp=1:CCDL=0:RTPL=0 # DRAM timing parameters = {nbk:tCCD:tRRD:tRCD:tRAS:tRP:tRC:CL:WL:tCDLR:tWR:nbkgrp:tCCDL:tRTPL}
-gpgpu_l2_rop_latency                  120 # ROP queue latency (default 85)
-dram_latency                         100 # DRAM latency (default 30)
-dram_dual_bus_interface                    0 # dual_bus_interface (default = 0) 
-dram_bnk_indexing_policy                    0 # dram_bnk_indexing_policy (0 = normal indexing, 1 = Xoring with the higher bits) (Default = 0)
-dram_bnkgrp_indexing_policy                    0 # dram_bnkgrp_indexing_policy (0 = take higher bits, 1 = take lower bits) (Default = 0)
-dram_seperate_write_queue_enable                    0 # Seperate_Write_Queue_Enable
-dram_write_queue_size             32:28:16 # Write_Queue_Size
-dram_elimnate_rw_turnaround                    0 # elimnate_rw_turnaround i.e set tWTR and tRTW = 0
-icnt_flit_size                        32 # icnt_flit_size
-gpgpu_mem_addr_mapping dramid@8;00000000.00000000.00000000.00000000.0000RRRR.RRRRRRRR.RBBBCCCC.BCCSSSSS # mapping memory address to dram model {dramid@<start bit>;<memory address map>}
-gpgpu_mem_addr_test                    0 # run sweep test to check address mapping for aliased address
-gpgpu_mem_address_mask                    1 # 0 = old addressing mask, 1 = new addressing mask, 2 = new add. mask + flipped bank sel and chip sel bits
-gpgpu_memory_partition_indexing                    0 # 0 = no indexing, 1 = bitwise xoring, 2 = IPoly, 3 = custom indexing
-accelwattch_xml_file accelwattch_sass_sim.xml # AccelWattch XML file
-power_simulation_enabled                    0 # Turn on power simulator (1=On, 0=Off)
-power_per_cycle_dump                    0 # Dump detailed power output each cycle
-hw_perf_file_name            hw_perf.csv # Hardware Performance Statistics file
-hw_perf_bench_name                       # Kernel Name in Hardware Performance Statistics file
-power_simulation_mode                    0 # Switch performance counter input for power simulation (0=Sim, 1=HW, 2=HW-Sim Hybrid)
-dvfs_enabled                           0 # Turn on DVFS for power model
-aggregate_power_stats                    0 # Accumulate power across all kernels
-accelwattch_hybrid_perfsim_L1_RH                    0 # Get L1 Read Hits for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L1_RM                    0 # Get L1 Read Misses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L1_WH                    0 # Get L1 Write Hits for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L1_WM                    0 # Get L1 Write Misses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L2_RH                    0 # Get L2 Read Hits for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L2_RM                    0 # Get L2 Read Misses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L2_WH                    0 # Get L2 Write Hits for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_L2_WM                    0 # Get L2 Write Misses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_CC_ACC                    0 # Get Constant Cache Acesses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_SHARED_ACC                    0 # Get Shared Memory Acesses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_DRAM_RD                    0 # Get DRAM Reads for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_DRAM_WR                    0 # Get DRAM Writes for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_NOC                    0 # Get Interconnect Acesses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_PIPE_DUTY                    0 # Get Pipeline Duty Cycle Acesses for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_NUM_SM_IDLE                    0 # Get Number of Idle SMs for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_CYCLES                    0 # Get Executed Cycles for Accelwattch-Hybrid from Accel-Sim
-accelwattch_hybrid_perfsim_VOLTAGE                    0 # Get Chip Voltage for Accelwattch-Hybrid from Accel-Sim
-power_trace_enabled                    0 # produce a file for the power trace (1=On, 0=Off)
-power_trace_zlevel                     6 # Compression level of the power trace output log (0=no comp, 9=highest)
-steady_power_levels_enabled                    0 # produce a file for the steady power levels (1=On, 0=Off)
-steady_state_definition                  8:4 # allowed deviation:number of samples
-gpgpu_max_cycle                        0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_insn                         0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_cta                          0 # terminates gpu simulation early (0 = no limit)
-gpgpu_max_completed_cta                    0 # terminates gpu simulation early (0 = no limit)
-gpgpu_runtime_stat                   500 # display runtime statistics such as dram utilization {<freq>:<flag>}
-liveness_message_freq                    1 # Minimum number of seconds between simulation liveness messages (0 = always print)
-gpgpu_compute_capability_major                    7 # Major compute capability version number
-gpgpu_compute_capability_minor                    0 # Minor compute capability version number
-gpgpu_flush_l1_cache                    0 # Flush L1 cache at the end of each kernel call
-gpgpu_flush_l2_cache                    0 # Flush L2 cache at the end of each kernel call
-gpgpu_deadlock_detect                    1 # Stop the simulation at deadlock (1=on (default), 0=off)
-gpgpu_ptx_instruction_classification                    0 # if enabled will classify ptx instruction types per kernel (Max 255 kernels now)
-gpgpu_ptx_sim_mode                     0 # Select between Performance (default) or Functional simulation (1)
-gpgpu_clock_domains 1607.0:1607.0:1607.0:2500.0 # Clock Domain Frequencies in MhZ {<Core Clock>:<ICNT Clock>:<L2 Clock>:<DRAM Clock>}
-gpgpu_max_concurrent_kernel                   32 # maximum kernels that can run concurrently on GPU, set this value according to max resident grids for your compute capability
-gpgpu_cflog_interval                    0 # Interval between each snapshot in control flow logger
-visualizer_enabled                     0 # Turn on visualizer output (1=On, 0=Off)
-visualizer_outputfile                 NULL # Specifies the output log file for visualizer
-visualizer_zlevel                      6 # Compression level of the visualizer output log (0=no comp, 9=highest)
-gpgpu_stack_size_limit                 1024 # GPU thread stack size
-gpgpu_heap_size_limit              8388608 # GPU malloc heap size 
-gpgpu_runtime_sync_depth_limit                    2 # GPU device runtime synchronize depth
-gpgpu_runtime_pending_launch_count_limit                 2048 # GPU device runtime pending launch count
-trace_enabled                          0 # Turn on traces
-trace_components                    none # comma seperated list of traces to enable. Complete list found in trace_streams.tup. Default none
-trace_sampling_core                    0 # The core which is printed using CORE_DPRINTF. Default 0
-trace_sampling_memory_partition                   -1 # The memory partition which is printed using MEMPART_DPRINTF. Default -1 (i.e. all)
-enable_ptx_file_line_stats                    1 # Turn on PTX source line statistic profiling. (1 = On)
-ptx_line_stats_filename gpgpu_inst_stats.txt # Output file for PTX source line statistics.
-gpgpu_kernel_launch_latency                    0 # Kernel launch latency in cycles. Default: 0
-gpgpu_cdp_enabled                      0 # Turn on CDP
-gpgpu_TB_launch_latency                    0 # thread block launch latency in cycles. Default: 0
-trace               /benchrun/accelsim-sass/sm6_gtx1080/vectoradd/input-1000000/results/traces/kernelslist.g # traces kernel filetraces kernel file directory
-trace_opcode_latency_initiation_int                  4,1 # Opcode latencies and initiation for integers in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_sp                  4,1 # Opcode latencies and initiation for sp in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_dp                  4,1 # Opcode latencies and initiation for dp in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_sfu                  4,1 # Opcode latencies and initiation for sfu in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_tensor                  4,1 # Opcode latencies and initiation for tensor in trace driven mode <latency,initiation>
-trace_opcode_latency_initiation_spec_op_1                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_2                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_3                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_4                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_5                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_6                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_7                  4,4 # specialized unit config <latency,initiation>
-trace_opcode_latency_initiation_spec_op_8                  4,4 # specialized unit config <latency,initiation>
DRAM Timing Options:
nbk                                    16 # number of banks
CCD                                     2 # column to column delay
RRD                                     6 # minimal delay between activation of rows in different banks
RCD                                    12 # row to column delay
RAS                                    28 # time needed to activate row
RP                                     12 # time needed to precharge (deactivate) row
RC                                     40 # row cycle time
CDLR                                    5 # switching from write to read (changes tWTR)
WR                                     12 # last data-in to row precharge
CL                                     12 # CAS latency
WL                                      4 # Write latency
nbkgrp                                  1 # number of bank groups
CCDL                                    0 # column to column delay between accesses to different bank groups
RTPL                                    0 # read to precharge delay between accesses to different bank groups
Total number of memory sub partition = 16
addr_dec_mask[CHIP]  = 0000000000000700 	high:11 low:8
addr_dec_mask[BK]    = 0000000000038080 	high:18 low:7
addr_dec_mask[ROW]   = 000000007ffc0000 	high:31 low:18
addr_dec_mask[COL]   = 000000000000787f 	high:15 low:0
addr_dec_mask[BURST] = 000000000000001f 	high:5 low:0
sub_partition_id_mask = 0000000000000080
GPGPU-Sim uArch: clock freqs: 1607000000.000000:1607000000.000000:1607000000.000000:2500000000.000000
GPGPU-Sim uArch: clock periods: 0.00000000062227753578:0.00000000062227753578:0.00000000062227753578:0.00000000040000000000
*** Initializing Memory Statistics ***
GPGPU-Sim uArch: interconnect node map (shaderID+MemID to icntID)
GPGPU-Sim uArch: Memory nodes ID start from index: 20
GPGPU-Sim uArch:    0   1   2   3   4   5   6
GPGPU-Sim uArch:    7   8   9  10  11  12  13
GPGPU-Sim uArch:   14  15  16  17  18  19  20
GPGPU-Sim uArch:   21  22  23  24  25  26  27
GPGPU-Sim uArch:   28  29  30  31  32  33  34
GPGPU-Sim uArch:   35  36  37  38  39  40  41
GPGPU-Sim uArch:   42  43  44  45  46  47  48
GPGPU-Sim uArch:   49
GPGPU-Sim uArch: interconnect node reverse map (icntID to shaderID+MemID)
GPGPU-Sim uArch: Memory nodes start from ID: 20
GPGPU-Sim uArch:    0   1   2   3   4   5   6
GPGPU-Sim uArch:    7   8   9  10  11  12  13
GPGPU-Sim uArch:   14  15  16  17  18  19  20
GPGPU-Sim uArch:   21  22  23  24  25  26  27
GPGPU-Sim uArch:   28  29  30  31  32  33  34
GPGPU-Sim uArch:   35  36  37  38  39  40  41
GPGPU-Sim uArch:   42  43  44  45  46  47  48
GPGPU-Sim uArch:   49
GPGPU-Sim uArch: performance model initialization complete.
launching memcpy command : MemcpyHtoD,0x00007f47dd600000,8000000
launching memcpy command : MemcpyHtoD,0x00007f47e4a00000,8000000
launching memcpy command : MemcpyHtoD,0x00007f47e5200000,8000000
Processing kernel /benchrun/accelsim-sass/sm6_gtx1080/vectoradd/input-1000000/results/traces/kernel-1.traceg
-kernel name = _Z6vecAddPdS_S_i
-kernel id = 1
-grid dim = (977,1,1)
-block dim = (1024,1,1)
-shmem = 0
-nregs = 10
-binary version = 61
-cuda stream id = 0
-shmem base_addr = 0x00007f480b000000
-local mem base_addr = 0x00007f4809000000
-nvbit version = 1.5.5
-accelsim tracer version = 3
Header info loaded for kernel command : /benchrun/accelsim-sass/sm6_gtx1080/vectoradd/input-1000000/results/traces/kernel-1.traceg
launching kernel name: _Z6vecAddPdS_S_i uid: 1
GPGPU-Sim uArch: Shader 0 bind to kernel 1 '_Z6vecAddPdS_S_i'
GPGPU-Sim uArch: CTA/core = 2, limited by: threads
thread block = 0,0,0
GPGPU-Sim uArch: Shader 1 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 1,0,0
GPGPU-Sim uArch: Shader 2 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 2,0,0
GPGPU-Sim uArch: Shader 3 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 3,0,0
GPGPU-Sim uArch: Shader 4 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 4,0,0
GPGPU-Sim uArch: Shader 5 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 5,0,0
GPGPU-Sim uArch: Shader 6 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 6,0,0
GPGPU-Sim uArch: Shader 7 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 7,0,0
GPGPU-Sim uArch: Shader 8 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 8,0,0
GPGPU-Sim uArch: Shader 9 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 9,0,0
GPGPU-Sim uArch: Shader 10 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 10,0,0
GPGPU-Sim uArch: Shader 11 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 11,0,0
GPGPU-Sim uArch: Shader 12 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 12,0,0
GPGPU-Sim uArch: Shader 13 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 13,0,0
GPGPU-Sim uArch: Shader 14 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 14,0,0
GPGPU-Sim uArch: Shader 15 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 15,0,0
GPGPU-Sim uArch: Shader 16 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 16,0,0
GPGPU-Sim uArch: Shader 17 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 17,0,0
GPGPU-Sim uArch: Shader 18 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 18,0,0
GPGPU-Sim uArch: Shader 19 bind to kernel 1 '_Z6vecAddPdS_S_i'
thread block = 19,0,0
thread block = 20,0,0
thread block = 21,0,0
thread block = 22,0,0
thread block = 23,0,0
thread block = 24,0,0
thread block = 25,0,0
thread block = 26,0,0
thread block = 27,0,0
thread block = 28,0,0
thread block = 29,0,0
thread block = 30,0,0
thread block = 31,0,0
thread block = 32,0,0
thread block = 33,0,0
thread block = 34,0,0
thread block = 35,0,0
thread block = 36,0,0
thread block = 37,0,0
thread block = 38,0,0
thread block = 39,0,0
thread block = 40,0,0
thread block = 41,0,0
thread block = 42,0,0
thread block = 43,0,0
thread block = 44,0,0
thread block = 45,0,0
thread block = 46,0,0
thread block = 47,0,0
thread block = 48,0,0
thread block = 49,0,0
thread block = 50,0,0
thread block = 51,0,0
thread block = 52,0,0
thread block = 53,0,0
thread block = 54,0,0
thread block = 55,0,0
thread block = 56,0,0
thread block = 57,0,0
thread block = 58,0,0
thread block = 59,0,0
thread block = 60,0,0
thread block = 61,0,0
thread block = 62,0,0
thread block = 63,0,0
thread block = 64,0,0
thread block = 65,0,0
thread block = 66,0,0
thread block = 67,0,0
thread block = 68,0,0
thread block = 69,0,0
thread block = 70,0,0
thread block = 71,0,0
thread block = 72,0,0
thread block = 73,0,0
thread block = 74,0,0
thread block = 75,0,0
thread block = 76,0,0
thread block = 77,0,0
thread block = 78,0,0
thread block = 79,0,0
thread block = 80,0,0
thread block = 81,0,0
thread block = 82,0,0
thread block = 83,0,0
thread block = 84,0,0
thread block = 85,0,0
thread block = 86,0,0
thread block = 87,0,0
thread block = 88,0,0
thread block = 89,0,0
thread block = 90,0,0
thread block = 91,0,0
thread block = 92,0,0
thread block = 93,0,0
thread block = 94,0,0
thread block = 95,0,0
thread block = 96,0,0
thread block = 97,0,0
thread block = 98,0,0
thread block = 99,0,0
thread block = 100,0,0
thread block = 101,0,0
thread block = 102,0,0
thread block = 103,0,0
thread block = 104,0,0
thread block = 105,0,0
thread block = 106,0,0
thread block = 107,0,0
thread block = 108,0,0
thread block = 109,0,0
thread block = 110,0,0
thread block = 111,0,0
thread block = 112,0,0
thread block = 113,0,0
thread block = 114,0,0
thread block = 115,0,0
thread block = 116,0,0
thread block = 117,0,0
thread block = 118,0,0
thread block = 119,0,0
thread block = 120,0,0
thread block = 121,0,0
thread block = 122,0,0
thread block = 123,0,0
thread block = 124,0,0
thread block = 125,0,0
thread block = 126,0,0
thread block = 127,0,0
thread block = 128,0,0
thread block = 129,0,0
thread block = 130,0,0
thread block = 131,0,0
thread block = 132,0,0
thread block = 133,0,0
thread block = 134,0,0
thread block = 135,0,0
thread block = 136,0,0
thread block = 137,0,0
thread block = 138,0,0
thread block = 139,0,0
thread block = 140,0,0
thread block = 141,0,0
thread block = 142,0,0
thread block = 143,0,0
thread block = 144,0,0
thread block = 145,0,0
thread block = 146,0,0
thread block = 147,0,0
thread block = 148,0,0
thread block = 149,0,0
thread block = 150,0,0
thread block = 151,0,0
thread block = 152,0,0
thread block = 153,0,0
thread block = 154,0,0
thread block = 155,0,0
thread block = 156,0,0
thread block = 157,0,0
thread block = 158,0,0
thread block = 159,0,0
thread block = 160,0,0
thread block = 161,0,0
thread block = 162,0,0
thread block = 163,0,0
thread block = 164,0,0
thread block = 165,0,0
thread block = 166,0,0
thread block = 167,0,0
thread block = 168,0,0
thread block = 169,0,0
thread block = 170,0,0
thread block = 171,0,0
thread block = 172,0,0
thread block = 173,0,0
thread block = 174,0,0
thread block = 175,0,0
thread block = 176,0,0
thread block = 177,0,0
thread block = 178,0,0
thread block = 179,0,0
thread block = 180,0,0
thread block = 181,0,0
thread block = 182,0,0
thread block = 183,0,0
thread block = 184,0,0
thread block = 185,0,0
thread block = 186,0,0
thread block = 187,0,0
thread block = 188,0,0
thread block = 189,0,0
thread block = 190,0,0
thread block = 191,0,0
thread block = 192,0,0
thread block = 193,0,0
thread block = 194,0,0
thread block = 195,0,0
thread block = 196,0,0
thread block = 197,0,0
thread block = 198,0,0
thread block = 199,0,0
thread block = 200,0,0
thread block = 201,0,0
thread block = 202,0,0
thread block = 203,0,0
thread block = 204,0,0
thread block = 205,0,0
thread block = 206,0,0
thread block = 207,0,0
thread block = 208,0,0
thread block = 209,0,0
thread block = 210,0,0
thread block = 211,0,0
thread block = 212,0,0
thread block = 213,0,0
thread block = 214,0,0
thread block = 215,0,0
thread block = 216,0,0
thread block = 217,0,0
thread block = 218,0,0
thread block = 219,0,0
thread block = 220,0,0
thread block = 221,0,0
thread block = 222,0,0
thread block = 223,0,0
thread block = 224,0,0
thread block = 225,0,0
thread block = 226,0,0
thread block = 227,0,0
thread block = 228,0,0
thread block = 229,0,0
thread block = 230,0,0
thread block = 231,0,0
thread block = 232,0,0
thread block = 233,0,0
thread block = 234,0,0
thread block = 235,0,0
thread block = 236,0,0
thread block = 237,0,0
thread block = 238,0,0
thread block = 239,0,0
thread block = 240,0,0
thread block = 241,0,0
thread block = 242,0,0
thread block = 243,0,0
thread block = 244,0,0
thread block = 245,0,0
thread block = 246,0,0
thread block = 247,0,0
thread block = 248,0,0
thread block = 249,0,0
thread block = 250,0,0
thread block = 251,0,0
thread block = 252,0,0
thread block = 253,0,0
thread block = 254,0,0
thread block = 255,0,0
thread block = 256,0,0
thread block = 257,0,0
thread block = 258,0,0
thread block = 259,0,0
thread block = 260,0,0
thread block = 261,0,0
thread block = 262,0,0
thread block = 263,0,0
thread block = 264,0,0
thread block = 265,0,0
thread block = 266,0,0
thread block = 267,0,0
thread block = 268,0,0
thread block = 269,0,0
thread block = 270,0,0
thread block = 271,0,0
thread block = 272,0,0
thread block = 273,0,0
thread block = 274,0,0
thread block = 275,0,0
thread block = 276,0,0
thread block = 277,0,0
thread block = 278,0,0
thread block = 279,0,0
thread block = 280,0,0
thread block = 281,0,0
thread block = 282,0,0
thread block = 283,0,0
thread block = 284,0,0
thread block = 285,0,0
thread block = 286,0,0
thread block = 287,0,0
thread block = 288,0,0
thread block = 289,0,0
thread block = 290,0,0
thread block = 291,0,0
thread block = 292,0,0
thread block = 293,0,0
thread block = 294,0,0
thread block = 295,0,0
thread block = 296,0,0
thread block = 297,0,0
thread block = 298,0,0
thread block = 299,0,0
thread block = 300,0,0
thread block = 301,0,0
thread block = 302,0,0
thread block = 303,0,0
thread block = 304,0,0
thread block = 305,0,0
thread block = 306,0,0
thread block = 307,0,0
thread block = 308,0,0
thread block = 309,0,0
thread block = 310,0,0
thread block = 311,0,0
thread block = 312,0,0
thread block = 313,0,0
thread block = 314,0,0
thread block = 315,0,0
thread block = 316,0,0
thread block = 317,0,0
thread block = 318,0,0
thread block = 319,0,0
thread block = 320,0,0
thread block = 321,0,0
thread block = 322,0,0
thread block = 323,0,0
thread block = 324,0,0
thread block = 325,0,0
thread block = 326,0,0
thread block = 327,0,0
thread block = 328,0,0
thread block = 329,0,0
thread block = 330,0,0
thread block = 331,0,0
thread block = 332,0,0
thread block = 333,0,0
thread block = 334,0,0
thread block = 335,0,0
thread block = 336,0,0
thread block = 337,0,0
thread block = 338,0,0
thread block = 339,0,0
thread block = 340,0,0
thread block = 341,0,0
thread block = 342,0,0
thread block = 343,0,0
thread block = 344,0,0
thread block = 345,0,0
thread block = 346,0,0
thread block = 347,0,0
thread block = 348,0,0
thread block = 349,0,0
thread block = 350,0,0
thread block = 351,0,0
thread block = 352,0,0
thread block = 353,0,0
thread block = 354,0,0
thread block = 355,0,0
thread block = 356,0,0
thread block = 357,0,0
thread block = 358,0,0
thread block = 359,0,0
thread block = 360,0,0
thread block = 361,0,0
thread block = 362,0,0
thread block = 363,0,0
thread block = 364,0,0
thread block = 365,0,0
thread block = 366,0,0
thread block = 367,0,0
thread block = 368,0,0
thread block = 369,0,0
thread block = 370,0,0
thread block = 371,0,0
thread block = 372,0,0
thread block = 373,0,0
thread block = 374,0,0
thread block = 375,0,0
thread block = 376,0,0
thread block = 377,0,0
thread block = 378,0,0
thread block = 379,0,0
thread block = 380,0,0
thread block = 381,0,0
thread block = 382,0,0
thread block = 383,0,0
thread block = 384,0,0
thread block = 385,0,0
thread block = 386,0,0
thread block = 387,0,0
thread block = 388,0,0
thread block = 389,0,0
thread block = 390,0,0
thread block = 391,0,0
thread block = 392,0,0
thread block = 393,0,0
thread block = 394,0,0
thread block = 395,0,0
thread block = 396,0,0
thread block = 397,0,0
thread block = 398,0,0
thread block = 399,0,0
thread block = 400,0,0
thread block = 401,0,0
thread block = 402,0,0
thread block = 403,0,0
thread block = 404,0,0
thread block = 405,0,0
thread block = 406,0,0
thread block = 407,0,0
thread block = 408,0,0
thread block = 409,0,0
thread block = 410,0,0
thread block = 411,0,0
thread block = 412,0,0
thread block = 413,0,0
thread block = 414,0,0
thread block = 415,0,0
thread block = 416,0,0
thread block = 417,0,0
thread block = 418,0,0
thread block = 419,0,0
thread block = 420,0,0
thread block = 421,0,0
thread block = 422,0,0
thread block = 423,0,0
thread block = 424,0,0
thread block = 425,0,0
thread block = 426,0,0
thread block = 427,0,0
thread block = 428,0,0
thread block = 429,0,0
thread block = 430,0,0
thread block = 431,0,0
thread block = 432,0,0
thread block = 433,0,0
thread block = 434,0,0
thread block = 435,0,0
thread block = 436,0,0
thread block = 437,0,0
thread block = 438,0,0
thread block = 439,0,0
thread block = 440,0,0
thread block = 441,0,0
thread block = 442,0,0
thread block = 443,0,0
thread block = 444,0,0
thread block = 445,0,0
thread block = 446,0,0
thread block = 447,0,0
thread block = 448,0,0
thread block = 449,0,0
thread block = 450,0,0
thread block = 451,0,0
thread block = 452,0,0
thread block = 453,0,0
thread block = 454,0,0
thread block = 455,0,0
thread block = 456,0,0
thread block = 457,0,0
thread block = 458,0,0
thread block = 459,0,0
thread block = 460,0,0
thread block = 461,0,0
thread block = 462,0,0
thread block = 463,0,0
thread block = 464,0,0
thread block = 465,0,0
thread block = 466,0,0
thread block = 467,0,0
thread block = 468,0,0
thread block = 469,0,0
thread block = 470,0,0
thread block = 471,0,0
thread block = 472,0,0
thread block = 473,0,0
thread block = 474,0,0
thread block = 475,0,0
thread block = 476,0,0
thread block = 477,0,0
thread block = 478,0,0
thread block = 479,0,0
thread block = 480,0,0
thread block = 481,0,0
thread block = 482,0,0
thread block = 483,0,0
thread block = 484,0,0
thread block = 485,0,0
thread block = 486,0,0
thread block = 487,0,0
thread block = 488,0,0
thread block = 489,0,0
thread block = 490,0,0
thread block = 491,0,0
thread block = 492,0,0
thread block = 493,0,0
thread block = 494,0,0
thread block = 495,0,0
thread block = 496,0,0
thread block = 497,0,0
thread block = 498,0,0
thread block = 499,0,0
thread block = 500,0,0
thread block = 501,0,0
thread block = 502,0,0
thread block = 503,0,0
thread block = 504,0,0
thread block = 505,0,0
thread block = 506,0,0
thread block = 507,0,0
thread block = 508,0,0
thread block = 509,0,0
thread block = 510,0,0
thread block = 511,0,0
thread block = 512,0,0
thread block = 513,0,0
thread block = 514,0,0
thread block = 515,0,0
thread block = 516,0,0
thread block = 517,0,0
thread block = 518,0,0
thread block = 519,0,0
thread block = 520,0,0
thread block = 521,0,0
thread block = 522,0,0
thread block = 523,0,0
thread block = 524,0,0
thread block = 525,0,0
thread block = 526,0,0
thread block = 527,0,0
thread block = 528,0,0
thread block = 529,0,0
thread block = 530,0,0
thread block = 531,0,0
thread block = 532,0,0
thread block = 533,0,0
thread block = 534,0,0
thread block = 535,0,0
thread block = 536,0,0
thread block = 537,0,0
thread block = 538,0,0
thread block = 539,0,0
thread block = 540,0,0
thread block = 541,0,0
thread block = 542,0,0
thread block = 543,0,0
thread block = 544,0,0
thread block = 545,0,0
thread block = 546,0,0
thread block = 547,0,0
thread block = 548,0,0
thread block = 549,0,0
thread block = 550,0,0
thread block = 551,0,0
thread block = 552,0,0
thread block = 553,0,0
thread block = 554,0,0
thread block = 555,0,0
thread block = 556,0,0
thread block = 557,0,0
thread block = 558,0,0
thread block = 559,0,0
thread block = 560,0,0
thread block = 561,0,0
thread block = 562,0,0
thread block = 563,0,0
thread block = 564,0,0
thread block = 565,0,0
thread block = 566,0,0
thread block = 567,0,0
thread block = 568,0,0
thread block = 569,0,0
thread block = 570,0,0
thread block = 571,0,0
thread block = 572,0,0
thread block = 573,0,0
thread block = 574,0,0
thread block = 575,0,0
thread block = 576,0,0
thread block = 577,0,0
thread block = 578,0,0
thread block = 579,0,0
thread block = 580,0,0
thread block = 581,0,0
thread block = 582,0,0
thread block = 583,0,0
thread block = 584,0,0
thread block = 585,0,0
thread block = 586,0,0
thread block = 587,0,0
thread block = 588,0,0
thread block = 589,0,0
thread block = 590,0,0
thread block = 591,0,0
thread block = 592,0,0
thread block = 593,0,0
thread block = 594,0,0
thread block = 595,0,0
thread block = 596,0,0
thread block = 597,0,0
thread block = 598,0,0
thread block = 599,0,0
thread block = 600,0,0
thread block = 601,0,0
thread block = 602,0,0
thread block = 603,0,0
thread block = 604,0,0
thread block = 605,0,0
thread block = 606,0,0
thread block = 607,0,0
thread block = 608,0,0
thread block = 609,0,0
thread block = 610,0,0
thread block = 611,0,0
thread block = 612,0,0
thread block = 613,0,0
thread block = 614,0,0
thread block = 615,0,0
thread block = 616,0,0
thread block = 617,0,0
thread block = 618,0,0
thread block = 619,0,0
thread block = 620,0,0
thread block = 621,0,0
thread block = 622,0,0
thread block = 623,0,0
thread block = 624,0,0
thread block = 625,0,0
thread block = 626,0,0
thread block = 627,0,0
thread block = 628,0,0
thread block = 629,0,0
thread block = 630,0,0
thread block = 631,0,0
thread block = 632,0,0
thread block = 633,0,0
thread block = 634,0,0
thread block = 635,0,0
thread block = 636,0,0
thread block = 637,0,0
thread block = 638,0,0
thread block = 639,0,0
thread block = 640,0,0
thread block = 641,0,0
thread block = 642,0,0
thread block = 643,0,0
thread block = 644,0,0
thread block = 645,0,0
thread block = 646,0,0
thread block = 647,0,0
thread block = 648,0,0
thread block = 649,0,0
thread block = 650,0,0
thread block = 651,0,0
thread block = 652,0,0
thread block = 653,0,0
thread block = 654,0,0
thread block = 655,0,0
thread block = 656,0,0
thread block = 657,0,0
thread block = 658,0,0
thread block = 659,0,0
thread block = 660,0,0
thread block = 661,0,0
thread block = 662,0,0
thread block = 663,0,0
thread block = 664,0,0
thread block = 665,0,0
thread block = 666,0,0
thread block = 667,0,0
thread block = 668,0,0
thread block = 669,0,0
thread block = 670,0,0
thread block = 671,0,0
thread block = 672,0,0
thread block = 673,0,0
thread block = 674,0,0
thread block = 675,0,0
thread block = 676,0,0
thread block = 677,0,0
thread block = 678,0,0
thread block = 679,0,0
thread block = 680,0,0
thread block = 681,0,0
thread block = 682,0,0
thread block = 683,0,0
thread block = 684,0,0
thread block = 685,0,0
thread block = 686,0,0
thread block = 687,0,0
thread block = 688,0,0
thread block = 689,0,0
thread block = 690,0,0
thread block = 691,0,0
thread block = 692,0,0
thread block = 693,0,0
thread block = 694,0,0
thread block = 695,0,0
thread block = 696,0,0
thread block = 697,0,0
thread block = 698,0,0
thread block = 699,0,0
thread block = 700,0,0
thread block = 701,0,0
thread block = 702,0,0
thread block = 703,0,0
thread block = 704,0,0
thread block = 705,0,0
thread block = 706,0,0
thread block = 707,0,0
thread block = 708,0,0
thread block = 709,0,0
thread block = 710,0,0
thread block = 711,0,0
thread block = 712,0,0
thread block = 713,0,0
thread block = 714,0,0
thread block = 715,0,0
thread block = 716,0,0
thread block = 717,0,0
thread block = 718,0,0
thread block = 719,0,0
thread block = 720,0,0
thread block = 721,0,0
thread block = 722,0,0
thread block = 723,0,0
thread block = 724,0,0
thread block = 725,0,0
thread block = 726,0,0
thread block = 727,0,0
thread block = 728,0,0
thread block = 729,0,0
thread block = 730,0,0
thread block = 731,0,0
thread block = 732,0,0
thread block = 733,0,0
thread block = 734,0,0
thread block = 735,0,0
thread block = 736,0,0
thread block = 737,0,0
thread block = 738,0,0
thread block = 739,0,0
thread block = 740,0,0
thread block = 741,0,0
thread block = 742,0,0
thread block = 743,0,0
thread block = 744,0,0
thread block = 745,0,0
thread block = 746,0,0
thread block = 747,0,0
thread block = 748,0,0
thread block = 749,0,0
thread block = 750,0,0
thread block = 751,0,0
thread block = 752,0,0
thread block = 753,0,0
thread block = 754,0,0
thread block = 755,0,0
thread block = 756,0,0
thread block = 757,0,0
thread block = 758,0,0
thread block = 759,0,0
thread block = 760,0,0
thread block = 761,0,0
thread block = 762,0,0
thread block = 763,0,0
thread block = 764,0,0
thread block = 765,0,0
thread block = 766,0,0
thread block = 767,0,0
thread block = 768,0,0
thread block = 769,0,0
thread block = 770,0,0
thread block = 771,0,0
thread block = 772,0,0
thread block = 773,0,0
thread block = 774,0,0
thread block = 775,0,0
thread block = 776,0,0
thread block = 777,0,0
thread block = 778,0,0
thread block = 779,0,0
thread block = 780,0,0
thread block = 781,0,0
thread block = 782,0,0
thread block = 783,0,0
thread block = 784,0,0
thread block = 785,0,0
thread block = 786,0,0
thread block = 787,0,0
thread block = 788,0,0
thread block = 789,0,0
thread block = 790,0,0
thread block = 791,0,0
thread block = 792,0,0
thread block = 793,0,0
thread block = 794,0,0
thread block = 795,0,0
thread block = 796,0,0
thread block = 797,0,0
thread block = 798,0,0
thread block = 799,0,0
thread block = 800,0,0
thread block = 801,0,0
thread block = 802,0,0
thread block = 803,0,0
thread block = 804,0,0
thread block = 805,0,0
thread block = 806,0,0
thread block = 807,0,0
thread block = 808,0,0
thread block = 809,0,0
thread block = 810,0,0
thread block = 811,0,0
thread block = 812,0,0
thread block = 813,0,0
thread block = 814,0,0
thread block = 815,0,0
thread block = 816,0,0
thread block = 817,0,0
thread block = 818,0,0
thread block = 819,0,0
thread block = 820,0,0
thread block = 821,0,0
thread block = 822,0,0
thread block = 823,0,0
thread block = 824,0,0
thread block = 825,0,0
thread block = 826,0,0
thread block = 827,0,0
thread block = 828,0,0
thread block = 829,0,0
thread block = 830,0,0
thread block = 831,0,0
thread block = 832,0,0
thread block = 833,0,0
thread block = 834,0,0
thread block = 835,0,0
thread block = 836,0,0
thread block = 837,0,0
thread block = 838,0,0
thread block = 839,0,0
thread block = 840,0,0
thread block = 841,0,0
thread block = 842,0,0
thread block = 843,0,0
thread block = 844,0,0
thread block = 845,0,0
thread block = 846,0,0
thread block = 847,0,0
thread block = 848,0,0
thread block = 849,0,0
thread block = 850,0,0
thread block = 851,0,0
thread block = 852,0,0
thread block = 853,0,0
thread block = 854,0,0
thread block = 855,0,0
thread block = 856,0,0
thread block = 857,0,0
thread block = 858,0,0
thread block = 859,0,0
thread block = 860,0,0
thread block = 861,0,0
thread block = 862,0,0
thread block = 863,0,0
thread block = 864,0,0
thread block = 865,0,0
thread block = 866,0,0
thread block = 867,0,0
thread block = 868,0,0
thread block = 869,0,0
thread block = 870,0,0
thread block = 871,0,0
thread block = 872,0,0
thread block = 873,0,0
thread block = 874,0,0
thread block = 875,0,0
thread block = 876,0,0
thread block = 877,0,0
thread block = 878,0,0
thread block = 879,0,0
thread block = 880,0,0
thread block = 881,0,0
thread block = 882,0,0
thread block = 883,0,0
thread block = 884,0,0
thread block = 885,0,0
thread block = 886,0,0
thread block = 887,0,0
thread block = 888,0,0
thread block = 889,0,0
thread block = 890,0,0
thread block = 891,0,0
thread block = 892,0,0
thread block = 893,0,0
thread block = 894,0,0
thread block = 895,0,0
thread block = 896,0,0
thread block = 897,0,0
thread block = 898,0,0
thread block = 899,0,0
thread block = 900,0,0
thread block = 901,0,0
thread block = 902,0,0
thread block = 903,0,0
thread block = 904,0,0
thread block = 905,0,0
thread block = 906,0,0
thread block = 907,0,0
thread block = 908,0,0
thread block = 909,0,0
thread block = 910,0,0
thread block = 911,0,0
thread block = 912,0,0
thread block = 913,0,0
thread block = 914,0,0
thread block = 915,0,0
thread block = 916,0,0
thread block = 917,0,0
thread block = 918,0,0
thread block = 919,0,0
thread block = 920,0,0
thread block = 921,0,0
thread block = 922,0,0
thread block = 923,0,0
thread block = 924,0,0
thread block = 925,0,0
thread block = 926,0,0
thread block = 927,0,0
thread block = 928,0,0
thread block = 929,0,0
thread block = 930,0,0
thread block = 931,0,0
thread block = 932,0,0
thread block = 933,0,0
thread block = 934,0,0
thread block = 935,0,0
thread block = 936,0,0
thread block = 937,0,0
thread block = 938,0,0
thread block = 939,0,0
thread block = 940,0,0
thread block = 941,0,0
thread block = 942,0,0
thread block = 943,0,0
thread block = 944,0,0
thread block = 945,0,0
thread block = 946,0,0
thread block = 947,0,0
thread block = 948,0,0
thread block = 949,0,0
thread block = 950,0,0
thread block = 951,0,0
thread block = 952,0,0
thread block = 953,0,0
thread block = 954,0,0
thread block = 955,0,0
thread block = 956,0,0
thread block = 957,0,0
thread block = 958,0,0
thread block = 959,0,0
thread block = 960,0,0
thread block = 961,0,0
thread block = 962,0,0
thread block = 963,0,0
thread block = 964,0,0
thread block = 965,0,0
thread block = 966,0,0
thread block = 967,0,0
thread block = 968,0,0
thread block = 969,0,0
thread block = 970,0,0
thread block = 971,0,0
thread block = 972,0,0
thread block = 973,0,0
thread block = 974,0,0
thread block = 975,0,0
thread block = 976,0,0
Destroy streams for kernel 1: size 0
kernel_name = _Z6vecAddPdS_S_i 
kernel_launch_uid = 1 
gpu_sim_cycle = 176302
gpu_sim_insn = 22004032
gpu_ipc =     124.8087
gpu_tot_sim_cycle = 176302
gpu_tot_sim_insn = 22004032
gpu_tot_ipc =     124.8087
gpu_tot_issued_cta = 977
gpu_occupancy = 67.6546% 
gpu_tot_occupancy = 67.6546% 
max_total_param_size = 0
gpu_stall_dramfull = 1159613
gpu_stall_icnt2sh    = 219110
partiton_level_parallism =       1.0637
partiton_level_parallism_total  =       1.0637
partiton_level_parallism_util =       2.0994
partiton_level_parallism_util_total  =       2.0994
L2_BW  =      54.7019 GB/Sec
L2_BW_total  =      54.7019 GB/Sec
gpu_total_sim_rate=196464

========= Core cache stats =========
L1I_cache:
	L1I_total_cache_accesses = 375070
	L1I_total_cache_misses = 1920
	L1I_total_cache_miss_rate = 0.0051
	L1I_total_cache_pending_hits = 0
	L1I_total_cache_reservation_fails = 9080
L1D_cache:
	L1D_cache_core[0]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[1]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[2]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[3]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[4]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[5]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[6]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[7]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[8]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[9]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[10]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[11]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[12]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[13]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[14]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[15]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[16]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[17]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[18]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_cache_core[19]: Access = 0, Miss = 0, Miss_rate = -nan, Pending_hits = 0, Reservation_fails = 0
	L1D_total_cache_accesses = 0
	L1D_total_cache_misses = 0
	L1D_total_cache_pending_hits = 0
	L1D_total_cache_reservation_fails = 0
	L1D_cache_data_port_util = 0.000
	L1D_cache_fill_port_util = 0.000
L1C_cache:
	L1C_total_cache_accesses = 0
	L1C_total_cache_misses = 0
	L1C_total_cache_pending_hits = 0
	L1C_total_cache_reservation_fails = 0
L1T_cache:
	L1T_total_cache_accesses = 0
	L1T_total_cache_misses = 0
	L1T_total_cache_pending_hits = 0
	L1T_total_cache_reservation_fails = 0

Total_core_cache_stats:
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][HIT] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][MISS] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][HIT] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][MISS] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][HIT] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][MISS] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[CONST_ACC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][HIT] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][MISS] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[TEXTURE_ACC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][HIT] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][MISS] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[GLOBAL_ACC_W][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][HIT] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][MISS] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[LOCAL_ACC_W][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][HIT] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][MISS] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[L1_WRBK_ACC][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][HIT] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][MISS] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[L2_WRBK_ACC][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][HIT] = 373150
	Total_core_cache_stats_breakdown[INST_ACC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][MISS] = 1920
	Total_core_cache_stats_breakdown[INST_ACC_R][RESERVATION_FAIL] = 9080
	Total_core_cache_stats_breakdown[INST_ACC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][MSHR_HIT] = 1880
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][HIT] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][MISS] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[L1_WR_ALLOC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][HIT] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][HIT_RESERVED] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][MISS] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][RESERVATION_FAIL] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][SECTOR_MISS] = 0
	Total_core_cache_stats_breakdown[L2_WR_ALLOC_R][MSHR_HIT] = 0
	Total_core_cache_stats_breakdown[INST_ACC_R][TOTAL_ACCESS] = 375070

Total_core_cache_fail_stats:
	Total_core_cache_fail_stats_breakdown[INST_ACC_R][MSHR_MERGE_ENRTY_FAIL] = 9080
ctas_completed 977, Shader 0 warp_id issue ditsribution:
warp_id:
0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 
distro:
624, 624, 624, 624, 624, 625, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 624, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 598, 
gpgpu_n_tot_thrd_icount = 23004032
gpgpu_n_tot_w_icount = 718876
gpgpu_n_stall_shd_mem = 3092807
gpgpu_n_mem_read_local = 0
gpgpu_n_mem_write_local = 0
gpgpu_n_mem_read_global = 125000
gpgpu_n_mem_write_global = 62500
gpgpu_n_mem_texture = 0
gpgpu_n_mem_const = 0
gpgpu_n_load_insn  = 2000000
gpgpu_n_store_insn = 1000000
gpgpu_n_shmem_insn = 0
gpgpu_n_sstarr_insn = 0
gpgpu_n_tex_insn = 0
gpgpu_n_const_mem_insn = 0
gpgpu_n_param_mem_insn = 0
gpgpu_n_shmem_bkconflict = 0
gpgpu_n_cache_bkconflict = 0
gpgpu_n_intrawarp_mshr_merge = 0
gpgpu_n_cmem_portconflict = 0
gpgpu_stall_shd_mem[c_mem][resource_stall] = 0
gpgpu_stall_shd_mem[s_mem][bk_conf] = 0
gpgpu_stall_shd_mem[gl_mem][resource_stall] = 0
gpgpu_stall_shd_mem[gl_mem][coal_stall] = 93750
gpgpu_stall_shd_mem[gl_mem][data_port_stall] = 0
gpu_reg_bank_conflict_stalls = 0
Warp Occupancy Distribution:
Stall:5016537	W0_Idle:256046	W0_Scoreboard:1150924	W1:0	W2:0	W3:0	W4:0	W5:0	W6:0	W7:0	W8:0	W9:0	W10:0	W11:0	W12:0	W13:0	W14:0	W15:0	W16:0	W17:0	W18:0	W19:0	W20:0	W21:0	W22:0	W23:0	W24:0	W25:0	W26:0	W27:0	W28:0	W29:0	W30:0	W31:0	W32:687626
single_issue_nums: WS0:265678	WS1:265684	
dual_issue_nums: WS0:46880	WS1:46877	
traffic_breakdown_coretomem[GLOBAL_ACC_R] = 1000000 {8:125000,}
traffic_breakdown_coretomem[GLOBAL_ACC_W] = 8500000 {136:62500,}
traffic_breakdown_coretomem[INST_ACC_R] = 320 {8:40,}
traffic_breakdown_memtocore[GLOBAL_ACC_R] = 17000000 {136:125000,}
traffic_breakdown_memtocore[GLOBAL_ACC_W] = 500000 {8:62500,}
traffic_breakdown_memtocore[INST_ACC_R] = 5440 {136:40,}
maxmflatency = 4735 
max_icnt2mem_latency = 1404 
maxmrqlatency = 3282 
max_icnt2sh_latency = 217 
averagemflatency = 1044 
avg_icnt2mem_latency = 200 
avg_mrq_latency = 327 
avg_icnt2sh_latency = 19 
mrq_lat_table:42062 	404 	2754 	5220 	12685 	21105 	30765 	36575 	43320 	36213 	16408 	2491 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
dq_lat_table:0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
mf_lat_table:0 	0 	0 	0 	0 	0 	0 	5 	18446 	93197 	65146 	10687 	19 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
icnt2mem_lat_table:0 	0 	0 	1599 	33211 	30395 	24608 	36187 	48271 	12722 	547 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
icnt2sh_lat_table:0 	0 	0 	106559 	56085 	20502 	4045 	309 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
mf_lat_pw_table:0 	0 	0 	0 	0 	0 	0 	0 	1 	143 	206 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	0 	
maximum concurrent accesses to same row:
dram[0]:        32        32        30        30        30        30        32        32        32        32        30        30        30        30        30        30 
dram[1]:        28        28        30        30        32        32        32        32        32        32        32        32        32        32        32        32 
dram[2]:        30        30        28        28        32        32        30        30        32        32        28        28        32        32        32        32 
dram[3]:        28        28        30        30        32        32        32        32        32        32        32        32        26        26        32        32 
dram[4]:        30        30        28        28        30        30        30        30        32        32        28        28        30        30        30        30 
dram[5]:        28        28        32        32        32        32        32        32        32        32        28        28        24        24        28        28 
dram[6]:        32        32        30        30        32        32        32        32        30        30        28        28        32        32        32        32 
dram[7]:        32        32        32        32        32        32        32        32        32        32        30        30        30        30        28        28 
maximum service time to same row:
dram[0]:      3701      3698      3294      3283      3127      3128      3601      3277      3471      3481      3209      3189      2864      2855      3146      3105 
dram[1]:      2836      2870      3604      3011      3075      3404      3542      3832      3955      4274      4164      4134      5047      5445      4549      4751 
dram[2]:      2944      4024      3011      3011      3701      3630      3256      3191      2959      3326      3216      3250      3291      3245      3464      3470 
dram[3]:      2780      2812      3086      3031      2916      2947      3137      3137      4096      4095      3832      3932      2854      2652      4429      4475 
dram[4]:      3827      3852      2679      2626      2906      2951      3425      3292      3007      3088      3097      3104      2998      3000      4445      4490 
dram[5]:      2657      2684      3350      3188      3290      3186      3304      3309      4632      4630      3857      3861      3014      2412      3083      3203 
dram[6]:      4142      4076      2973      2938      4048      4038      3721      3708      3071      3070      3004      3040      3445      3441      4201      4370 
dram[7]:      3118      3124      3091      3097      3098      3146      3375      3380      4320      4321      4301      3746      3079      2823      3611      3610 
average row accesses per activate:
dram[0]:  3.861868  4.034553  3.334454  3.444444  3.438475  3.555556  3.420690  3.574775  3.468582  3.577778  3.221477  3.276450  3.595506  3.786982  3.522936  3.678161 
dram[1]:  3.708411  3.890196  3.633700  3.729323  3.674074  3.928713  3.561939  3.736346  3.474820  3.617978  3.194675  3.368421  3.380282  3.699422  3.434705  3.720930 
dram[2]:  3.480702  3.574775  3.568345  3.793499  3.486819  3.627057  3.530249  3.701493  3.644613  3.743689  3.344948  3.535912  3.404255  3.728155  3.685221  3.832335 
dram[3]:  3.536542  3.653775  3.312187  3.607273  3.647059  3.822736  3.837524  3.867446  3.364747  3.486438  3.304647  3.380282  3.484574  3.602251  3.497268  3.595506 
dram[4]:  3.647059  3.736346  3.607273  3.640367  3.574775  3.667283  3.722327  3.633700  3.461400  3.603738  3.575419  3.664122  3.368421  3.535912  3.629490  3.720930 
dram[5]:  3.667283  3.771863  3.391453  3.511504  3.613843  3.750473  3.694600  3.757576  3.218698  3.234899  3.410302  3.522936  3.210702  3.368421  3.779528  3.794466 
dram[6]:  3.568345  3.715356  3.462478  3.549195  3.815385  3.936508  3.379898  3.561939  3.570370  3.817822  3.270869  3.380282  3.588785  3.582090  3.728155  3.847695 
dram[7]:  3.408935  3.607273  3.594203  3.647059  3.486819  3.694600  3.674074  3.852427  3.382456  3.524680  3.356643  3.434705  3.356643  3.643264  3.287671  3.327556 
average row locality = 250002/70054 = 3.568704
number of total memory accesses made:
dram[0]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[1]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[2]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[3]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[4]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[5]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[6]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
dram[7]:         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0 
total accesses: 0
min_bank_accesses = 0!
min_chip_accesses = 0!
number of total read accesses:
dram[0]:      5956      5956      5952      5952      5952      5952      5952      5952      5796      5796      5760      5760      5760      5760      5760      5760 
dram[1]:      5952      5952      5952      5952      5952      5952      5952      5952      5796      5796      5760      5760      5760      5760      5760      5760 
dram[2]:      5952      5952      5952      5952      5952      5952      5952      5952      5784      5784      5760      5760      5760      5760      5760      5760 
dram[3]:      5952      5952      5952      5952      5952      5952      5952      5952      5784      5784      5760      5760      5760      5760      5760      5760 
dram[4]:      5952      5952      5952      5952      5952      5952      5952      5952      5784      5784      5760      5760      5760      5760      5760      5760 
dram[5]:      5952      5952      5952      5952      5952      5952      5952      5952      5784      5784      5760      5760      5760      5760      5760      5760 
dram[6]:      5952      5952      5952      5952      5952      5952      5952      5952      5784      5784      5760      5760      5760      5760      5760      5760 
dram[7]:      5952      5952      5952      5952      5952      5952      5952      5952      5784      5784      5760      5760      5760      5760      5760      5760 
total dram reads = 750008
bank skew: 5956/5760 = 1.03
chip skew: 93776/93744 = 1.00
number of total write accesses:
dram[0]:      1984      1984      1984      1984      1984      1984      1984      1984      1932      1932      1920      1920      1920      1920      1920      1920 
dram[1]:      1984      1984      1984      1984      1984      1984      1984      1984      1932      1932      1920      1920      1920      1920      1920      1920 
dram[2]:      1984      1984      1984      1984      1984      1984      1984      1984      1928      1928      1920      1920      1920      1920      1920      1920 
dram[3]:      1984      1984      1984      1984      1984      1984      1984      1984      1928      1928      1920      1920      1920      1920      1920      1920 
dram[4]:      1984      1984      1984      1984      1984      1984      1984      1984      1928      1928      1920      1920      1920      1920      1920      1920 
dram[5]:      1984      1984      1984      1984      1984      1984      1984      1984      1928      1928      1920      1920      1920      1920      1920      1920 
dram[6]:      1984      1984      1984      1984      1984      1984      1984      1984      1928      1928      1920      1920      1920      1920      1920      1920 
dram[7]:      1984      1984      1984      1984      1984      1984      1984      1984      1928      1928      1920      1920      1920      1920      1920      1920 
total dram writes = 250000
bank skew: 1984/1920 = 1.03
chip skew: 31256/31248 = 1.00
average mf latency per bank:
dram[0]:        210       220       207       218       201       211       192       203       193       204       195       208       216       226       213       220
dram[1]:        174       180       172       178       177       186       168       176       168       174       172       180       173       183       173       186
dram[2]:        202       203       200       204       196       201       194       202       190       197       193       200       202       212       202       209
dram[3]:        179       188       180       186       183       190       181       184       172       178       176       180       184       184       181       186
dram[4]:        196       199       201       207       202       209       198       203       189       196       193       199       208       216       198       209
dram[5]:        185       190       180       186       181       186       183       188       174       179       181       184       181       188       184       189
dram[6]:        215       223       224       233       222       232       212       222       221       227       215       222       233       238       229       239
dram[7]:        181       188       187       189       176       185       186       191       176       183       184       186       180       185       183       186
maximum mf latency per bank:
dram[0]:       3961      3997      4106      4098      3913      3944      3928      4007      3995      4003      3547      3616      3705      3781      3878      3885
dram[1]:       3349      3376      3395      3922      3446      3500      3914      3935      3989      3996      4032      4494      3097      3550      3798      3932
dram[2]:       3955      3992      3980      4250      3849      3865      3634      3699      4272      3647      3341      3552      4043      4102      3890      3904
dram[3]:       3434      3449      4301      4094      3175      3321      4059      4429      3486      3453      3392      3618      3375      3433      3326      3413
dram[4]:       3741      3828      3669      3694      3857      3945      3458      3621      3549      3756      3855      3642      3467      3493      3277      3404
dram[5]:       3398      3487      4353      4027      3806      3858      4064      4092      3936      4115      3590      3789      3302      3613      3115      3122
dram[6]:       4011      4167      3867      4114      4118      4735      4183      3946      3842      3831      3976      4001      3738      3698      3849      3868
dram[7]:       4021      4016      3516      3612      3756      3776      3769      3784      3756      3813      3590      3764      3359      3569      3304      3308
Memory Partition 0: 
Cache L2_bank_000:
MSHR contents

Cache L2_bank_001:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[0]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=274271 n_nop=134652 n_act=8834 n_pre=8818 n_ref_event=0 n_req=31258 n_rd=62520 n_rd_L2_A=31256 n_write=31256 n_wr_bk=0 bw_util=0.9117
n_activity=269594 dram_eff=0.9276
bk0: 5956a 80157i bk1: 5956a 74608i bk2: 5952a 78980i bk3: 5952a 73440i bk4: 5952a 83532i bk5: 5952a 78889i bk6: 5952a 81502i bk7: 5952a 76908i bk8: 5796a 85196i bk9: 5796a 80116i bk10: 5760a 85650i bk11: 5760a 80566i bk12: 5760a 82672i bk13: 5760a 78123i bk14: 5760a 85255i bk15: 5760a 82714i 

------------------------------------------------------------------------

Row_Buffer_Locality = 0.717384
Row_Buffer_Locality_read = 0.704487
Row_Buffer_Locality_write = 0.756079
Bank_Level_Parallism = 11.612182
Bank_Level_Parallism_Col = 11.100016
Bank_Level_Parallism_Ready = 5.355109
write_to_read_ratio_blp_rw_average = 0.501320
GrpLevelPara = 1.000000 

BW Util details:
bwutil = 0.911741 
total_CMD = 274271 
util_bw = 250064 
Wasted_Col = 19242 
Wasted_Row = 126 
Idle = 4839 

BW Util Bottlenecks: 
RCDc_limit = 5108 
RCDWRc_limit = 1175 
WTRc_limit = 88245 
RTWc_limit = 95607 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 88245 
RTWc_limit_alone = 95607 

Commands details: 
total_CMD = 274271 
n_nop = 134652 
Read = 62520 
Write = 31256 
L2_Alloc = 31256 
L2_WB = 0 
n_act = 8834 
n_pre = 8818 
n_ref = 0 
n_req = 31258 
total_req = 125032 

Dual Bus Interface Util: 
issued_total_row = 17652 
issued_total_col = 125032 
Row_Bus_Util =  0.064360 
CoL_Bus_Util = 0.455870 
Either_Row_CoL_Bus_Util = 0.509055 
Issued_on_Two_Bus_Simul_Util = 0.011175 
issued_two_Eff = 0.021953 
queue_avg = 60.398571 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=64 avg=60.3986
Memory Partition 1: 
Cache L2_bank_002:
MSHR contents

Cache L2_bank_003:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[1]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=274271 n_nop=134986 n_act=8679 n_pre=8663 n_ref_event=0 n_req=31256 n_rd=62512 n_rd_L2_A=31256 n_write=31256 n_wr_bk=0 bw_util=0.9117
n_activity=270820 dram_eff=0.9233
bk0: 5952a 90952i bk1: 5952a 83977i bk2: 5952a 87452i bk3: 5952a 82148i bk4: 5952a 86770i bk5: 5952a 79656i bk6: 5952a 85569i bk7: 5952a 80671i bk8: 5796a 90463i bk9: 5796a 86775i bk10: 5760a 92071i bk11: 5760a 86376i bk12: 5760a 89309i bk13: 5760a 84073i bk14: 5760a 97140i bk15: 5760a 88711i 

------------------------------------------------------------------------

Row_Buffer_Locality = 0.722325
Row_Buffer_Locality_read = 0.711074
Row_Buffer_Locality_write = 0.756079
Bank_Level_Parallism = 11.182425
Bank_Level_Parallism_Col = 10.683120
Bank_Level_Parallism_Ready = 5.259458
write_to_read_ratio_blp_rw_average = 0.492089
GrpLevelPara = 1.000000 

BW Util details:
bwutil = 0.911682 
total_CMD = 274271 
util_bw = 250048 
Wasted_Col = 20356 
Wasted_Row = 209 
Idle = 3658 

BW Util Bottlenecks: 
RCDc_limit = 5436 
RCDWRc_limit = 1265 
WTRc_limit = 86070 
RTWc_limit = 95661 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 86070 
RTWc_limit_alone = 95661 

Commands details: 
total_CMD = 274271 
n_nop = 134986 
Read = 62512 
Write = 31256 
L2_Alloc = 31256 
L2_WB = 0 
n_act = 8679 
n_pre = 8663 
n_ref = 0 
n_req = 31256 
total_req = 125024 

Dual Bus Interface Util: 
issued_total_row = 17342 
issued_total_col = 125024 
Row_Bus_Util =  0.063229 
CoL_Bus_Util = 0.455841 
Either_Row_CoL_Bus_Util = 0.507837 
Issued_on_Two_Bus_Simul_Util = 0.011233 
issued_two_Eff = 0.022120 
queue_avg = 55.291855 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=64 avg=55.2919
Memory Partition 2: 
Cache L2_bank_004:
MSHR contents

Cache L2_bank_005:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[2]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=274271 n_nop=135148 n_act=8680 n_pre=8664 n_ref_event=0 n_req=31248 n_rd=62496 n_rd_L2_A=31248 n_write=31248 n_wr_bk=0 bw_util=0.9114
n_activity=269306 dram_eff=0.9283
bk0: 5952a 82791i bk1: 5952a 77787i bk2: 5952a 83583i bk3: 5952a 79709i bk4: 5952a 82357i bk5: 5952a 76504i bk6: 5952a 82279i bk7: 5952a 77889i bk8: 5784a 89137i bk9: 5784a 82196i bk10: 5760a 88905i bk11: 5760a 81947i bk12: 5760a 88505i bk13: 5760a 82615i bk14: 5760a 85199i bk15: 5760a 81122i 

------------------------------------------------------------------------

Row_Buffer_Locality = 0.722222
Row_Buffer_Locality_read = 0.709080
Row_Buffer_Locality_write = 0.761649
Bank_Level_Parallism = 11.500258
Bank_Level_Parallism_Col = 10.992542
Bank_Level_Parallism_Ready = 5.286835
write_to_read_ratio_blp_rw_average = 0.503470
GrpLevelPara = 1.000000 

BW Util details:
bwutil = 0.911449 
total_CMD = 274271 
util_bw = 249984 
Wasted_Col = 19039 
Wasted_Row = 157 
Idle = 5091 

BW Util Bottlenecks: 
RCDc_limit = 4791 
RCDWRc_limit = 1124 
WTRc_limit = 86139 
RTWc_limit = 94157 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 86139 
RTWc_limit_alone = 94157 

Commands details: 
total_CMD = 274271 
n_nop = 135148 
Read = 62496 
Write = 31248 
L2_Alloc = 31248 
L2_WB = 0 
n_act = 8680 
n_pre = 8664 
n_ref = 0 
n_req = 31248 
total_req = 124992 

Dual Bus Interface Util: 
issued_total_row = 17344 
issued_total_col = 124992 
Row_Bus_Util =  0.063237 
CoL_Bus_Util = 0.455724 
Either_Row_CoL_Bus_Util = 0.507246 
Issued_on_Two_Bus_Simul_Util = 0.011715 
issued_two_Eff = 0.023095 
queue_avg = 59.950611 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=64 avg=59.9506
Memory Partition 3: 
Cache L2_bank_006:
MSHR contents

Cache L2_bank_007:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[3]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=274271 n_nop=134716 n_act=8788 n_pre=8772 n_ref_event=0 n_req=31248 n_rd=62496 n_rd_L2_A=31248 n_write=31248 n_wr_bk=0 bw_util=0.9114
n_activity=271074 dram_eff=0.9222
bk0: 5952a 90489i bk1: 5952a 83818i bk2: 5952a 91790i bk3: 5952a 85492i bk4: 5952a 90146i bk5: 5952a 85508i bk6: 5952a 86449i bk7: 5952a 82263i bk8: 5784a 91826i bk9: 5784a 87616i bk10: 5760a 96145i bk11: 5760a 91619i bk12: 5760a 95144i bk13: 5760a 91782i bk14: 5760a 94599i bk15: 5760a 88364i 

------------------------------------------------------------------------

Row_Buffer_Locality = 0.718766
Row_Buffer_Locality_read = 0.709037
Row_Buffer_Locality_write = 0.747952
Bank_Level_Parallism = 11.021676
Bank_Level_Parallism_Col = 10.521128
Bank_Level_Parallism_Ready = 5.195850
write_to_read_ratio_blp_rw_average = 0.489965
GrpLevelPara = 1.000000 

BW Util details:
bwutil = 0.911449 
total_CMD = 274271 
util_bw = 249984 
Wasted_Col = 20611 
Wasted_Row = 236 
Idle = 3440 

BW Util Bottlenecks: 
RCDc_limit = 5642 
RCDWRc_limit = 1263 
WTRc_limit = 85845 
RTWc_limit = 95048 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 85845 
RTWc_limit_alone = 95048 

Commands details: 
total_CMD = 274271 
n_nop = 134716 
Read = 62496 
Write = 31248 
L2_Alloc = 31248 
L2_WB = 0 
n_act = 8788 
n_pre = 8772 
n_ref = 0 
n_req = 31248 
total_req = 124992 

Dual Bus Interface Util: 
issued_total_row = 17560 
issued_total_col = 124992 
Row_Bus_Util =  0.064024 
CoL_Bus_Util = 0.455724 
Either_Row_CoL_Bus_Util = 0.508822 
Issued_on_Two_Bus_Simul_Util = 0.010927 
issued_two_Eff = 0.021475 
queue_avg = 55.208794 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=64 avg=55.2088
Memory Partition 4: 
Cache L2_bank_008:
MSHR contents

Cache L2_bank_009:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[4]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=274271 n_nop=135060 n_act=8656 n_pre=8640 n_ref_event=0 n_req=31248 n_rd=62496 n_rd_L2_A=31248 n_write=31248 n_wr_bk=0 bw_util=0.9114
n_activity=269433 dram_eff=0.9278
bk0: 5952a 83272i bk1: 5952a 79672i bk2: 5952a 84664i bk3: 5952a 79785i bk4: 5952a 83820i bk5: 5952a 79373i bk6: 5952a 81906i bk7: 5952a 78897i bk8: 5784a 87142i bk9: 5784a 82719i bk10: 5760a 90999i bk11: 5760a 87204i bk12: 5760a 85557i bk13: 5760a 81493i bk14: 5760a 87534i bk15: 5760a 80987i 

------------------------------------------------------------------------

Row_Buffer_Locality = 0.722990
Row_Buffer_Locality_read = 0.713731
Row_Buffer_Locality_write = 0.750768
Bank_Level_Parallism = 11.445940
Bank_Level_Parallism_Col = 10.950567
Bank_Level_Parallism_Ready = 5.351087
write_to_read_ratio_blp_rw_average = 0.496365
GrpLevelPara = 1.000000 

BW Util details:
bwutil = 0.911449 
total_CMD = 274271 
util_bw = 249984 
Wasted_Col = 19097 
Wasted_Row = 199 
Idle = 4991 

BW Util Bottlenecks: 
RCDc_limit = 4905 
RCDWRc_limit = 1193 
WTRc_limit = 86890 
RTWc_limit = 93457 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 86890 
RTWc_limit_alone = 93457 

Commands details: 
total_CMD = 274271 
n_nop = 135060 
Read = 62496 
Write = 31248 
L2_Alloc = 31248 
L2_WB = 0 
n_act = 8656 
n_pre = 8640 
n_ref = 0 
n_req = 31248 
total_req = 124992 

Dual Bus Interface Util: 
issued_total_row = 17296 
issued_total_col = 124992 
Row_Bus_Util =  0.063062 
CoL_Bus_Util = 0.455724 
Either_Row_CoL_Bus_Util = 0.507567 
Issued_on_Two_Bus_Simul_Util = 0.011219 
issued_two_Eff = 0.022103 
queue_avg = 60.002716 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=64 avg=60.0027
Memory Partition 5: 
Cache L2_bank_010:
MSHR contents
MSHR: tag=0xe599dd00, atomic=0 1 entries : 0x555e2d5a45a0 :  mf: uid=634150, sid03:w61, part=5, addr=0x7f47e599dd00, load , size=128, unknown  status = IN_PARTITION_DRAM (176299), 

Cache L2_bank_011:
MSHR contents
MSHR: tag=0xe599dd80, atomic=0 1 entries : 0x555e2d85df60 :  mf: uid=634149, sid03:w61, part=5, addr=0x7f47e599dd80, load , size=128, unknown  status = IN_PARTITION_DRAM (176301), 

In Dram Latency Queue (total = 0): 
DRAM[5]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=274271 n_nop=134712 n_act=8845 n_pre=8829 n_ref_event=0 n_req=31248 n_rd=62496 n_rd_L2_A=31245 n_write=31248 n_wr_bk=0 bw_util=0.9114
n_activity=270995 dram_eff=0.9224
bk0: 5952a 83697i bk1: 5952a 80493i bk2: 5952a 86284i bk3: 5952a 81368i bk4: 5952a 88756i bk5: 5952a 82102i bk6: 5952a 86885i bk7: 5949a 81132i bk8: 5784a 88145i bk9: 5784a 83660i bk10: 5760a 91877i bk11: 5760a 87553i bk12: 5760a 94500i bk13: 5760a 88079i bk14: 5760a 93747i bk15: 5760a 88769i 

------------------------------------------------------------------------

Row_Buffer_Locality = 0.716942
Row_Buffer_Locality_read = 0.706648
Row_Buffer_Locality_write = 0.747824
Bank_Level_Parallism = 11.191194
Bank_Level_Parallism_Col = 10.681845
Bank_Level_Parallism_Ready = 5.257641
write_to_read_ratio_blp_rw_average = 0.492011
GrpLevelPara = 1.000000 

BW Util details:
bwutil = 0.911427 
total_CMD = 274271 
util_bw = 249978 
Wasted_Col = 20663 
Wasted_Row = 180 
Idle = 3450 

BW Util Bottlenecks: 
RCDc_limit = 5910 
RCDWRc_limit = 1310 
WTRc_limit = 87003 
RTWc_limit = 96510 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 87003 
RTWc_limit_alone = 96510 

Commands details: 
total_CMD = 274271 
n_nop = 134712 
Read = 62496 
Write = 31248 
L2_Alloc = 31245 
L2_WB = 0 
n_act = 8845 
n_pre = 8829 
n_ref = 0 
n_req = 31248 
total_req = 124989 

Dual Bus Interface Util: 
issued_total_row = 17674 
issued_total_col = 124989 
Row_Bus_Util =  0.064440 
CoL_Bus_Util = 0.455714 
Either_Row_CoL_Bus_Util = 0.508836 
Issued_on_Two_Bus_Simul_Util = 0.011317 
issued_two_Eff = 0.022241 
queue_avg = 55.539291 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=64 avg=55.5393
Memory Partition 6: 
Cache L2_bank_012:
MSHR contents

Cache L2_bank_013:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[6]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=274271 n_nop=134904 n_act=8675 n_pre=8659 n_ref_event=0 n_req=31248 n_rd=62496 n_rd_L2_A=31248 n_write=31248 n_wr_bk=0 bw_util=0.9114
n_activity=269931 dram_eff=0.9261
bk0: 5952a 83756i bk1: 5952a 81038i bk2: 5952a 84992i bk3: 5952a 82503i bk4: 5952a 82608i bk5: 5952a 77725i bk6: 5952a 85704i bk7: 5952a 82305i bk8: 5784a 89037i bk9: 5784a 85030i bk10: 5760a 87225i bk11: 5760a 85435i bk12: 5760a 86619i bk13: 5760a 84143i bk14: 5760a 86857i bk15: 5760a 82813i 

------------------------------------------------------------------------

Row_Buffer_Locality = 0.722382
Row_Buffer_Locality_read = 0.707843
Row_Buffer_Locality_write = 0.766001
Bank_Level_Parallism = 11.372275
Bank_Level_Parallism_Col = 10.872047
Bank_Level_Parallism_Ready = 5.248444
write_to_read_ratio_blp_rw_average = 0.502935
GrpLevelPara = 1.000000 

BW Util details:
bwutil = 0.911449 
total_CMD = 274271 
util_bw = 249984 
Wasted_Col = 19767 
Wasted_Row = 84 
Idle = 4436 

BW Util Bottlenecks: 
RCDc_limit = 5739 
RCDWRc_limit = 1207 
WTRc_limit = 88336 
RTWc_limit = 97754 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 88336 
RTWc_limit_alone = 97754 

Commands details: 
total_CMD = 274271 
n_nop = 134904 
Read = 62496 
Write = 31248 
L2_Alloc = 31248 
L2_WB = 0 
n_act = 8675 
n_pre = 8659 
n_ref = 0 
n_req = 31248 
total_req = 124992 

Dual Bus Interface Util: 
issued_total_row = 17334 
issued_total_col = 124992 
Row_Bus_Util =  0.063200 
CoL_Bus_Util = 0.455724 
Either_Row_CoL_Bus_Util = 0.508136 
Issued_on_Two_Bus_Simul_Util = 0.010789 
issued_two_Eff = 0.021232 
queue_avg = 60.858345 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=64 avg=60.8583
Memory Partition 7: 
Cache L2_bank_014:
MSHR contents

Cache L2_bank_015:
MSHR contents

In Dram Latency Queue (total = 0): 
DRAM[7]: 16 bks, busW=4 BL=8 CL=12, tRRD=6 tCCD=2, tRCD=12 tRAS=28 tRP=12 tRC=40
n_cmd=274271 n_nop=134687 n_act=8897 n_pre=8881 n_ref_event=0 n_req=31248 n_rd=62496 n_rd_L2_A=31248 n_write=31248 n_wr_bk=0 bw_util=0.9114
n_activity=270848 dram_eff=0.923
bk0: 5952a 90648i bk1: 5952a 84513i bk2: 5952a 86589i bk3: 5952a 81759i bk4: 5952a 89010i bk5: 5952a 80583i bk6: 5952a 84538i bk7: 5952a 80966i bk8: 5784a 91479i bk9: 5784a 85986i bk10: 5760a 89201i bk11: 5760a 87376i bk12: 5760a 91885i bk13: 5760a 88640i bk14: 5760a 91327i bk15: 5760a 87579i 

------------------------------------------------------------------------

Row_Buffer_Locality = 0.715278
Row_Buffer_Locality_read = 0.701186
Row_Buffer_Locality_write = 0.757553
Bank_Level_Parallism = 11.179846
Bank_Level_Parallism_Col = 10.667108
Bank_Level_Parallism_Ready = 5.226375
write_to_read_ratio_blp_rw_average = 0.491713
GrpLevelPara = 1.000000 

BW Util details:
bwutil = 0.911449 
total_CMD = 274271 
util_bw = 249984 
Wasted_Col = 20493 
Wasted_Row = 214 
Idle = 3580 

BW Util Bottlenecks: 
RCDc_limit = 5787 
RCDWRc_limit = 1195 
WTRc_limit = 86135 
RTWc_limit = 96801 
CCDLc_limit = 0 
rwq = 0 
CCDLc_limit_alone = 0 
WTRc_limit_alone = 86135 
RTWc_limit_alone = 96801 

Commands details: 
total_CMD = 274271 
n_nop = 134687 
Read = 62496 
Write = 31248 
L2_Alloc = 31248 
L2_WB = 0 
n_act = 8897 
n_pre = 8881 
n_ref = 0 
n_req = 31248 
total_req = 124992 

Dual Bus Interface Util: 
issued_total_row = 17778 
issued_total_col = 124992 
Row_Bus_Util =  0.064819 
CoL_Bus_Util = 0.455724 
Either_Row_CoL_Bus_Util = 0.508927 
Issued_on_Two_Bus_Simul_Util = 0.011616 
issued_two_Eff = 0.022825 
queue_avg = 55.714878 


dram_util_bins: 0 0 0 0 0 0 0 0 0 0
dram_eff_bins: 0 0 0 0 0 0 0 0 0 0
mrqq: max=64 avg=55.7149

========= L2 cache stats =========
L2_cache_bank[0]: Access = 11741, Miss = 11722, Miss_rate = 0.998, Pending_hits = 19, Reservation_fails = 161
L2_cache_bank[1]: Access = 11741, Miss = 11722, Miss_rate = 0.998, Pending_hits = 19, Reservation_fails = 183
L2_cache_bank[2]: Access = 11721, Miss = 11721, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 93
L2_cache_bank[3]: Access = 11721, Miss = 11721, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 95
L2_cache_bank[4]: Access = 11718, Miss = 11718, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 139
L2_cache_bank[5]: Access = 11718, Miss = 11718, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 135
L2_cache_bank[6]: Access = 11718, Miss = 11718, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 166
L2_cache_bank[7]: Access = 11718, Miss = 11718, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 126
L2_cache_bank[8]: Access = 11718, Miss = 11718, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 175
L2_cache_bank[9]: Access = 11718, Miss = 11718, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 178
L2_cache_bank[10]: Access = 11718, Miss = 11718, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 135
L2_cache_bank[11]: Access = 11718, Miss = 11718, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 123
L2_cache_bank[12]: Access = 11718, Miss = 11718, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 176
L2_cache_bank[13]: Access = 11718, Miss = 11718, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 150
L2_cache_bank[14]: Access = 11718, Miss = 11718, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 148
L2_cache_bank[15]: Access = 11718, Miss = 11718, Miss_rate = 1.000, Pending_hits = 0, Reservation_fails = 104
L2_total_cache_accesses = 187540
L2_total_cache_misses = 187502
L2_total_cache_miss_rate = 0.9998
L2_total_cache_pending_hits = 38
L2_total_cache_reservation_fails = 2287
L2_total_cache_breakdown:
	L2_cache_stats_breakdown[GLOBAL_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_R][MISS] = 125000
	L2_cache_stats_breakdown[GLOBAL_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][MISS] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][MISS] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[CONST_ACC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][MISS] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[TEXTURE_ACC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_W][HIT] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_W][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_W][MISS] = 62500
	L2_cache_stats_breakdown[GLOBAL_ACC_W][RESERVATION_FAIL] = 2287
	L2_cache_stats_breakdown[GLOBAL_ACC_W][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_W][MSHR_HIT] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][HIT] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][MISS] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[LOCAL_ACC_W][MSHR_HIT] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][HIT] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][MISS] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[L1_WRBK_ACC][MSHR_HIT] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][HIT] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][MISS] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[L2_WRBK_ACC][MSHR_HIT] = 0
	L2_cache_stats_breakdown[INST_ACC_R][HIT] = 0
	L2_cache_stats_breakdown[INST_ACC_R][HIT_RESERVED] = 38
	L2_cache_stats_breakdown[INST_ACC_R][MISS] = 2
	L2_cache_stats_breakdown[INST_ACC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[INST_ACC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[INST_ACC_R][MSHR_HIT] = 38
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][HIT] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][MISS] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[L1_WR_ALLOC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][HIT] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][HIT_RESERVED] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][MISS] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][RESERVATION_FAIL] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][SECTOR_MISS] = 0
	L2_cache_stats_breakdown[L2_WR_ALLOC_R][MSHR_HIT] = 0
	L2_cache_stats_breakdown[GLOBAL_ACC_R][TOTAL_ACCESS] = 125000
	L2_cache_stats_breakdown[GLOBAL_ACC_W][TOTAL_ACCESS] = 62500
	L2_cache_stats_breakdown[INST_ACC_R][TOTAL_ACCESS] = 40
L2_total_cache_reservation_fail_breakdown:
	L2_cache_stats_fail_breakdown[GLOBAL_ACC_W][MISS_QUEUE_FULL] = 2287
L2_cache_data_port_util = 0.000
L2_cache_fill_port_util = 0.266

icnt_total_pkts_mem_to_simt=687700
icnt_total_pkts_simt_to_mem=437540
LD_mem_lat_dist  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
ST_mem_lat_dist  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
----------------------------Interconnect-DETAILS--------------------------------
Class 0:
Packet latency average = 94.7698
	minimum = 6
	maximum = 1335
Network latency average = 56.3744
	minimum = 6
	maximum = 1176
Slowest packet = 2259
Flit latency average = 34.0686
	minimum = 6
	maximum = 1176
Slowest flit = 3316
Fragmentation average = 0.851669
	minimum = 0
	maximum = 933
Injected packet rate average = 0.0425497
	minimum = 0 (at node 36)
	maximum = 0.066596 (at node 20)
Accepted packet rate average = 0.0425497
	minimum = 0 (at node 36)
	maximum = 0.066596 (at node 20)
Injected flit rate average = 0.127649
	minimum = 0 (at node 36)
	maximum = 0.244336 (at node 20)
Accepted flit rate average= 0.127649
	minimum = 0 (at node 36)
	maximum = 0.203707 (at node 3)
Injected packet length average = 3
Accepted packet length average = 3
Total in-flight flits = 0 (0 measured)
====== Overall Traffic Statistics ======
====== Traffic class 0 ======
Packet latency average = 94.7698 (1 samples)
	minimum = 6 (1 samples)
	maximum = 1335 (1 samples)
Network latency average = 56.3744 (1 samples)
	minimum = 6 (1 samples)
	maximum = 1176 (1 samples)
Flit latency average = 34.0686 (1 samples)
	minimum = 6 (1 samples)
	maximum = 1176 (1 samples)
Fragmentation average = 0.851669 (1 samples)
	minimum = 0 (1 samples)
	maximum = 933 (1 samples)
Injected packet rate average = 0.0425497 (1 samples)
	minimum = 0 (1 samples)
	maximum = 0.066596 (1 samples)
Accepted packet rate average = 0.0425497 (1 samples)
	minimum = 0 (1 samples)
	maximum = 0.066596 (1 samples)
Injected flit rate average = 0.127649 (1 samples)
	minimum = 0 (1 samples)
	maximum = 0.244336 (1 samples)
Accepted flit rate average = 0.127649 (1 samples)
	minimum = 0 (1 samples)
	maximum = 0.203707 (1 samples)
Injected packet size average = 3 (1 samples)
Accepted packet size average = 3 (1 samples)
Hops average = 1 (1 samples)
----------------------------END-of-Interconnect-DETAILS-------------------------


gpgpu_simulation_time = 0 days, 0 hrs, 1 min, 52 sec (112 sec)
gpgpu_simulation_rate = 196464 (inst/sec)
gpgpu_simulation_rate = 1574 (cycle/sec)
gpgpu_silicon_slowdown = 1020965x
GPGPU-Sim: *** simulation thread exiting ***
GPGPU-Sim: *** exit detected ***
